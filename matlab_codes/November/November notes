-> only sto least squares gradient and nesterov
-> kai sto 1/m

--> 11/11/2019 <--

-keep the residual and condition number stable 
	change the dimensions
	
-keep the residual and dimension stable 
	change the condition number

-keep the dimension stable and condittion number 
	change the residual


-->12/11/2019<--

FOR GOOD CONDITION NUMBER
-WE know the direction of gradient (see contour lines)
Thus, we can use it to take a good stochastic direction.

-LMS algorithm steps

--> 13/11/2019 <--

check the error and window scheme

--> 14/11/2019 <--

to do: check which stochastic gradients are good

We observed that when we have oscillations (around the optimum maybe) (often in small condition number) we had to adapt the window scheme in order to decrease the stepsize
However in larger condition numbers, we observed that we had a plateau (very slow decrease of cost function) thus an adaptation for a larger stepsize should be implemented.

-->15/11/2019<--
	

check again for the step with L
using as initial step in backtracking 1/L(overestimated).

-->18/11/2019<--

->using an estimation for L~norm(A_t_b) we have better resutls for small condtion numbers and relative small noise (0.01)
->We have noticed that t = 1 doesnot deacrease it remains constant, so we nedd a bigger initial step
-> (norm(mu*randn(n,1) - 2*A_t_b))/norm(randn(n,1)) needs revision. for small condtion numbers we have good performance

